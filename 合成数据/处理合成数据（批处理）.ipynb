{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbbd2314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入必要的包\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import icalendar\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "from chinese_calendar import is_holiday\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c116b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "函数名：load_data\n",
    "功  能：读取文件，并以DataFrame形式载入数据\n",
    "参  数：file_path :文件路径\n",
    "        encode :编码方式，默认‘gbk’\n",
    "输  出：df:DataFrame格式的文件数据\n",
    "\n",
    "'''\n",
    "def load_data(file_path, encode = 'gbk'):\n",
    "    df = pd.read_csv(file_path ,encoding=encode)\n",
    "    columns = df.columns\n",
    "    df.fillna(df.mean(numeric_only=True) ,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "785c0d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "函数名：remove_data\n",
    "功  能：清洗生成数据中'05:00-10:00', '10:00-15:00', '15:00-19:00', '19:00-23:00'\n",
    "        四个时段的数据，使一天的用餐数据不超过4条\n",
    "参  数：df:DataFrame格式的文件数据\n",
    "输  出：filtered_df:DataFrame格式的文件数据\n",
    "\n",
    "'''\n",
    "\n",
    "def remove_multi(df):\n",
    "    # 将日期和时间合并为一个datetime列，并按日期进行排序\n",
    "    df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
    "    df = df.sort_values(by='datetime', ascending=False).drop(['date', 'time'], axis=1)\n",
    "    \n",
    "    # 按日期分组，并计算每天的数据条数\n",
    "    grouped = df.groupby(df['datetime'].dt.date).size().reset_index(name='count')\n",
    "    \n",
    "    # 处理数据\n",
    "    for index, row in grouped.iterrows():\n",
    "        date = row['datetime']\n",
    "        count = row['count']\n",
    "\n",
    "        # 获取当天的所有数据\n",
    "        day_data = df[df['datetime'].dt.date == date]\n",
    "\n",
    "        # 按时间段随机保留一条数据\n",
    "        time_slots = ['05:00-10:00', '10:00-15:00', '15:00-19:00', '19:00-23:00']\n",
    "        for slot in time_slots:\n",
    "            slot_data = day_data[day_data['datetime'].dt.strftime('%H:%M').between(slot[:5], slot[6:])]\n",
    "            if len(slot_data) > 1:\n",
    "                # 随机选择一条数据保留\n",
    "                index_to_keep = random.choice(slot_data.index)\n",
    "                df = df.drop(slot_data.index.drop(index_to_keep))\n",
    "                \n",
    "    # 过滤掉“00:00-09:00”时间段中消费大于10元的数据\n",
    "    filtered_df = df.copy()\n",
    "\n",
    "    time_slot = filtered_df['datetime'].dt.strftime('%H:%M').between('05:00', '09:00')\n",
    "    amount_greater = filtered_df['amount'] > 10\n",
    "\n",
    "    filtered_df = filtered_df[~(time_slot & amount_greater)]\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dca59d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "函数名：add_col\n",
    "功  能：根据datetime添加week和holiday特征列\n",
    "参  数：df:DataFrame格式的文件数据\n",
    "输  出：filtered_df:DataFrame格式的文件数据\n",
    "\n",
    "'''\n",
    "\n",
    "def add_col(df):\n",
    "    filtered_df = df.copy()\n",
    "    \n",
    "    # 提取日期和时间信息\n",
    "    filtered_df['datetime'] = pd.to_datetime(filtered_df['datetime'])\n",
    "    filtered_df['date'] = filtered_df['datetime'].dt.date\n",
    "    filtered_df['time'] = filtered_df['datetime'].dt.time\n",
    "\n",
    "    # 添加\"week\"列\n",
    "    filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n",
    "    filtered_df['week'] = filtered_df['date'].dt.weekday + 1  # 星期一为1，星期日为7\n",
    "    \n",
    "    #添加\"holiday\"列\n",
    "    holiday = []\n",
    "\n",
    "    for date in filtered_df['date'] :\n",
    "        if is_holiday(date):\n",
    "            holiday.append(1)\n",
    "        else :\n",
    "            holiday.append(0)\n",
    "\n",
    "    filtered_df['holiday'] = holiday\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "687a7939",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "函数名：add_weather\n",
    "功  能：加入climate天气数据\n",
    "参  数：df:DataFrame格式的文件数据\n",
    "输  出：filtered_df:DataFrame格式的文件数据\n",
    "\n",
    "'''\n",
    "\n",
    "def add_weather(df):\n",
    "    filtered_df = df.copy()\n",
    "    \n",
    "    trade_times = filtered_df['time'].astype(str)\n",
    "    processed_times = []\n",
    "\n",
    "    for time_str in trade_times:\n",
    "        # 将时间字符串转换为datetime对象\n",
    "        dt = datetime.strptime(time_str, \"%H:%M:%S\")\n",
    "        # 如果分钟大于等于30，则小时进一位，并将分钟置为0\n",
    "        if dt.minute >= 30:\n",
    "            dt = dt.replace(hour=dt.hour + 1, minute=0)\n",
    "        else:\n",
    "            dt = dt.replace(minute=0)\n",
    "        # 将处理后的时间添加到列表中\n",
    "        processed_times.append(dt.strftime(\"%H:%M\"))\n",
    "\n",
    "    filtered_df['时间_1'] = processed_times\n",
    "    filtered_df['datetime'] = filtered_df['date'].astype(str) + ' ' + filtered_df['时间_1'].astype(str).str[:2]\n",
    "    \n",
    "    #读入climate天气数据\n",
    "    climate = pd.read_csv('../temp/climate.csv' ,encoding='gbk')\n",
    "\n",
    "    #处理climate使其datetime特征与filtered_df对应\n",
    "    climate['时间'] = climate['时间'].str.zfill(5)\n",
    "    climate['date'] = pd.to_datetime(climate['date'])\n",
    "    climate['datetime'] = climate['date'].astype(str) + ' ' + climate['时间'].str[:2]\n",
    "    climate.rename(columns={'date': '日期'}, inplace=True)\n",
    "    \n",
    "    #合并数据\n",
    "    filtered_df = pd.merge(filtered_df, climate, on='datetime')\n",
    "    filtered_df = filtered_df.drop(['datetime','日期','时间','重要天象','时间_1'],axis=1)\n",
    "    \n",
    "    #将特殊天气转换为0/1\n",
    "    w_list = []\n",
    "    weather = filtered_df['特殊天象'].values\n",
    "\n",
    "    for w in weather:\n",
    "        if str(w) == 'nan':\n",
    "            w_list.append(0)\n",
    "        else:\n",
    "            w_list.append(1)\n",
    "    filtered_df['特殊天象'] = w_list\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caad7373",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "函数名：transform_time\n",
    "功  能：根据date和time特征，生成时间戳timestamp列，并将date转换为%Y-%m-%d格式\n",
    "参  数：df:DataFrame格式的文件数据\n",
    "输  出：filtered_df:DataFrame格式的文件数据\n",
    "\n",
    "'''\n",
    "\n",
    "def transform_time(df):\n",
    "    filtered_df = df.copy()\n",
    "    \n",
    "    # 将日期和时间合并为新的datetime列\n",
    "    filtered_df['timestamp'] = pd.to_datetime(filtered_df['date'].astype(str) + ' ' + filtered_df['time'].astype(str))\n",
    "\n",
    "    # 将datetime转换为浮点数时间戳\n",
    "    filtered_df['timestamp'] = filtered_df['timestamp'].apply(lambda x: x.timestamp())\n",
    "    \n",
    "    column_mapping = {\n",
    "        'amount': '交易额',\n",
    "        'merchant_name': '商户名称',\n",
    "        'date': '交易日期',\n",
    "        'time': '交易时间',\n",
    "        'week': '星期',\n",
    "        'holiday': '节假日',\n",
    "        'timestamp': '时间戳'\n",
    "    }\n",
    "\n",
    "    # 使用rename()方法重命名DataFrame的列\n",
    "    filtered_df.rename(columns=column_mapping, inplace=True)\n",
    "    \n",
    "    filtered_df['交易日期'] = pd.to_datetime(filtered_df['交易日期'])\n",
    "    filtered_df['交易日期'] = filtered_df['交易日期'].dt.strftime(\"%Y/%m/%d\")\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d36c75fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "函数名：add_course\n",
    "功  能：加入course课程数据\n",
    "参  数：df:DataFrame格式的文件数据\n",
    "        id:生成数据的类别\n",
    "输  出：filtered_df:DataFrame格式的文件数据\n",
    "\n",
    "'''\n",
    "\n",
    "def add_course(df, id):\n",
    "    filtered_df = df.copy()\n",
    "    \n",
    "    course_folder = '../temp/final'\n",
    "    course_folder = os.path.join(course_folder, id)\n",
    "    \n",
    "    #空的DataFrame\n",
    "    course_df = pd.DataFrame()\n",
    "    \n",
    "    # 循环读取课表文件并拼接\n",
    "    for coursename in os.listdir(course_folder):\n",
    "        if coursename.endswith(\".csv\") and coursename[0].isdigit():  # 以数字开头且为CSV文件\n",
    "            coursepath = os.path.join(course_folder, coursename)\n",
    "            course = load_data(coursepath)\n",
    "            course_df = pd.concat([course_df, course])\n",
    "    \n",
    "    #将课表中的Weekday数字化       \n",
    "    week = []\n",
    "    for row in course_df['Weekday']:\n",
    "        if row == 'Monday':\n",
    "            week.append(1)\n",
    "        elif row == 'Tuesday':\n",
    "            week.append(2)\n",
    "        elif row == 'Wednesday':\n",
    "            week.append(3)\n",
    "        elif row == 'Thursday':\n",
    "            week.append(4)\n",
    "        elif row == 'Friday':\n",
    "            week.append(5)\n",
    "        elif row == 'Saturday':\n",
    "            week.append(6)\n",
    "        elif row == 'Sunday':\n",
    "            week.append(7)\n",
    "    course_df['Weekday'] = week\n",
    "    \n",
    "    # 将日期字符串转换为日期格式\n",
    "    filtered_df['交易日期'] = pd.to_datetime(filtered_df['交易日期'], format=\"%Y/%m/%d\")\n",
    "    course_df['Start Date'] = pd.to_datetime(course_df['Start Date'], format=\"%Y-%m-%d\")\n",
    "    course_df['End Date'] = pd.to_datetime(course_df['End Date'], format=\"%Y-%m-%d\")\n",
    "    \n",
    "    #添加课程特征列，默认值为0\n",
    "    filtered_df['饭点课程_早'] = 0\n",
    "    filtered_df['饭点课程_午'] = 0\n",
    "    filtered_df['饭点课程_晚'] = 0\n",
    "    \n",
    "    # 使用条件索引进行高效筛选和修改\n",
    "    for _, row_c in course_df.iterrows():\n",
    "        mask = (filtered_df['交易日期'].between(row_c['Start Date'], row_c['End Date'])) & (filtered_df['星期'] == row_c['Weekday'])\n",
    "        if row_c['Start Time'] == '8:00' or row_c['Start Time'] == '08:00':\n",
    "            filtered_df.loc[mask, '饭点课程_早'] = 1\n",
    "        if row_c['End Time'] == '11:45':\n",
    "            filtered_df.loc[mask, '饭点课程_午'] = 1\n",
    "        if row_c['End Time'] == '17:25' or row_c['End Time'] == '17:30':\n",
    "            filtered_df.loc[mask, '饭点课程_晚'] = 1\n",
    "            \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bf01997",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "函数名：transform_merchant\n",
    "功  能：添加merchant_code列，将商户名称和编号一一对应，同时将所有列名转换为英文\n",
    "参  数：df:DataFrame格式的文件数据\n",
    "输  出：filtered_df:DataFrame格式的文件数据\n",
    "\n",
    "'''\n",
    "\n",
    "def transform_merchant(df):\n",
    "    filtered_df = df.copy()\n",
    "    \n",
    "    #读取map映射文件\n",
    "    map_path = '../数据处理/map.csv'\n",
    "    map = load_data(map_path)\n",
    "    \n",
    "    # 将商户编号映射到原始数据框 df 中，使用 '商户名称' 列作为合并键\n",
    "    filtered_df = filtered_df.merge(map, on='商户名称', how='left')\n",
    "\n",
    "    column_mapping = {\n",
    "        '交易额': 'amount',\n",
    "        '商户名称': 'merchant_name',\n",
    "        '交易日期': 'date',\n",
    "        '交易时间': 'time',\n",
    "        '星期': 'weekday',\n",
    "        '节假日': 'holiday',\n",
    "        '气温': 'temperature',\n",
    "        '气象站大气压': 'station_pressure',\n",
    "        '海平面大气压': 'sea_level_pressure',\n",
    "        '相对湿度': 'relative_humidity',\n",
    "        '平均风速': 'windspeed',\n",
    "        '特殊天象': 'special_weather',\n",
    "        '时间戳': 'timestamp',\n",
    "        '饭点课程_早': 'meal_course_breakfast',\n",
    "        '饭点课程_午': 'meal_course_lunch',\n",
    "        '饭点课程_晚': 'meal_course_dinner',\n",
    "        '商户编号': 'merchant_code'\n",
    "    }\n",
    "\n",
    "    # 使用rename()方法重命名DataFrame的列\n",
    "    filtered_df.rename(columns=column_mapping, inplace=True)\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "783b3742",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "函数名：draw_plot\n",
    "功  能：将部分数据归一化，并绘制plot折线图，存贮在img目录下\n",
    "参  数：df:DataFrame格式的文件数据\n",
    "        id:生成数据的类别\n",
    "        output_folder:存储图片的路径\n",
    "        file_name:处理文件的名称\n",
    "输  出：synth_df:DataFrame格式的文件数据\n",
    "\n",
    "'''\n",
    "\n",
    "def draw_plot(df, id, output_folder, file_name):\n",
    "    synth_df = df.copy()\n",
    "    \n",
    "    #获取真实数据\n",
    "    real_folder = '../temp/final'\n",
    "    real_path = os.path.join(real_folder, id, 'final(english).csv')\n",
    "    real_df = load_data(real_path)\n",
    "    \n",
    "    # 选择需要归一化的列（如果有多个列）\n",
    "    columns_to_normalize = ['amount', 'temperature', 'station_pressure', 'sea_level_pressure', 'relative_humidity', 'windspeed',\n",
    "                           'timestamp']\n",
    "\n",
    "    # 初始化 MinMaxScaler\n",
    "    real_scaler = MinMaxScaler()\n",
    "    synth_scaler = MinMaxScaler()\n",
    "\n",
    "    # 对需要归一化的列进行归一化操作\n",
    "    real_df[columns_to_normalize] = real_scaler.fit_transform(real_df[columns_to_normalize])\n",
    "    synth_df[columns_to_normalize] = synth_scaler.fit_transform(synth_df[columns_to_normalize])\n",
    "    \n",
    "    cols = [\"amount\", \"temperature\", \"station_pressure\", \"sea_level_pressure\", \n",
    "            \"relative_humidity\", \"windspeed\",\"special_weather\", \"timestamp\"\n",
    "    ]\n",
    "\n",
    "    real_data_plot = real_df[cols]\n",
    "    synth_data_plot = synth_df[cols]\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(15, 10))\n",
    "    axes=axes.flatten()\n",
    "\n",
    "    for j, col in enumerate(cols):\n",
    "        df = pd.DataFrame({'Real': real_data_plot.iloc[:20, j],\n",
    "                       'Synthetic': synth_data_plot.iloc[:20, j]})\n",
    "        df.plot(ax=axes[j],\n",
    "                title = col,\n",
    "                secondary_y='Synthetic data', style=['-', '--'])\n",
    "    fig.tight_layout()\n",
    "\n",
    "    save_path = os.path.join(output_folder, 'img')\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    png_filename = f\"{file_name}.png\"\n",
    "    save_path = os.path.join(save_path, png_filename)\n",
    "    plt.savefig(save_path, dpi=200)\n",
    "    \n",
    "    # 关闭绘图窗口，不显示图形\n",
    "    plt.close()\n",
    "    \n",
    "    return synth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72ac10ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dataset 代表特征数据\n",
    "start_index 代表从数据的第几个索引值开始取\n",
    "history_size 滑动窗口大小\n",
    "end_index 代表数据取到哪个索引就结束\n",
    "target_size 代表选取未来某一时间点还是时间段\n",
    "step 代表在滑动窗口中每隔多少步取一组特征\n",
    "'''\n",
    " \n",
    "def TimeSeries(dataset, start_index, history_size, end_index, step,\n",
    "               target_size):\n",
    "    \n",
    "    data = []  # 保存特征数据\n",
    "    \n",
    "    start_index = start_index + history_size  # 第一次的取值范围[0:start_index]\n",
    "    \n",
    "    # 如果没有指定滑动窗口取到哪个结束，那就取到最后\n",
    "    if end_index is None:\n",
    "        # 数据集最后一块是用来作为标签值的，特征不能取到底\n",
    "        end_index = len(dataset) - target_size\n",
    "        \n",
    "    # 滑动窗口的起始位置到终止位置每次移动一步\n",
    "    for i in range(start_index, end_index):\n",
    "        \n",
    "        # 滑窗中的值不全部取出来用，每隔60min取一次\n",
    "        index = range(i-history_size, i, step)  # 第一次相当于range(0, start_index, 4)\n",
    "        \n",
    "        # 根据索引取出所有的特征数据的指定行\n",
    "        data.append(dataset.iloc[index])\n",
    "    \n",
    "    # 返回划分好了的时间序列特征\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b4bcbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "函数名：PCA_tSNE\n",
    "功  能：将数据通过PCA和t-SNE降维后绘制对比图，然后存贮在img目录下\n",
    "参  数：df:DataFrame格式的文件数据\n",
    "        id:生成数据的类别\n",
    "        output_folder:存储图片的路径\n",
    "        file_name:处理文件的名称\n",
    "输  出：无\n",
    "\n",
    "'''\n",
    "\n",
    "def PCA_tSNE(df, id, output_folder, file_name):\n",
    "    synth_df = df.copy()\n",
    "    \n",
    "    #获取真实数据\n",
    "    real_folder = '../temp/final'\n",
    "    real_path = os.path.join(real_folder, id, 'final(english).csv')\n",
    "    real_df = load_data(real_path)\n",
    "    \n",
    "    #获取用于降维的特征\n",
    "    feature_to_analyse = [\"amount\", \"weekday\", \"temperature\", \"station_pressure\",\n",
    "                          \"sea_level_pressure\", \"relative_humidity\", \"windspeed\",\n",
    "                          \"timestamp\",\"merchant_code\"\n",
    "    ]\n",
    "    \n",
    "     # 选择需要归一化的列（如果有多个列）\n",
    "    columns_to_normalize = ['amount', 'temperature', 'station_pressure', 'sea_level_pressure', 'relative_humidity', 'windspeed',\n",
    "                           'timestamp']\n",
    "\n",
    "    # 初始化 MinMaxScaler\n",
    "    real_scaler = MinMaxScaler()\n",
    "\n",
    "    # 对需要归一化的列进行归一化操作\n",
    "    real_df[columns_to_normalize] = real_scaler.fit_transform(real_df[columns_to_normalize])\n",
    "\n",
    "    real_data = real_df[feature_to_analyse]\n",
    "    synth_data = synth_df[feature_to_analyse]\n",
    "    \n",
    "    # 从真实数据中随机抽取与合成数据行数相同数量的数据\n",
    "    if len(real_data) > len(synth_data):\n",
    "        length = len(synth_data)\n",
    "        real_data = real_data.sample(n=length, replace=False)\n",
    "    else:\n",
    "        length = len(real_data)\n",
    "        synth_data = synth_data.sample(n=length, replace=False)\n",
    "    sample = length\n",
    "    history_size = 8\n",
    "    target_size =  0\n",
    "    step = 1  \n",
    "\n",
    "    real_data =  TimeSeries(dataset=real_data, start_index=0, history_size=history_size, end_index=sample,\n",
    "                        step=step, target_size=target_size)\n",
    "\n",
    "    synth_data =  TimeSeries(dataset=synth_data, start_index=0, history_size=history_size, end_index=sample,\n",
    "                        step=step, target_size=target_size)\n",
    "    \n",
    "    real_data_reduced = real_data.reshape(-1, history_size) #(?, 9)\n",
    "    synth_data_reduced = synth_data.reshape(-1,history_size) #(?, 9)\n",
    "    \n",
    "    shape = real_data_reduced.shape[0]\n",
    "\n",
    "    n_components = 2\n",
    "    pca = PCA(n_components=n_components)\n",
    "    tsne = TSNE(n_components=n_components, n_iter=300)\n",
    "\n",
    "    pca.fit(real_data_reduced)\n",
    "\n",
    "    pca_real = pd.DataFrame(pca.transform(real_data_reduced))\n",
    "    pca_synth = pd.DataFrame(pca.transform(synth_data_reduced))\n",
    "\n",
    "    data_reduced = np.concatenate((real_data_reduced, synth_data_reduced), axis=0)\n",
    "    tsne_results = pd.DataFrame(tsne.fit_transform(data_reduced))\n",
    "    \n",
    "    fig = plt.figure(constrained_layout=True, figsize=(20, 10))\n",
    "    spec = gridspec.GridSpec(ncols=2, nrows=1, figure=fig)\n",
    "\n",
    "    ax = fig.add_subplot(spec[0,0])\n",
    "    ax.set_title('PCA results',\n",
    "                 fontsize=20,\n",
    "                 color='red',\n",
    "                 pad=10)\n",
    "\n",
    "    # PCA 散点图\n",
    "    plt.scatter(pca_real.iloc[:, 0].values, pca_real.iloc[:, 1].values,\n",
    "                c='black', alpha=0.5, label='Original', s=100)\n",
    "    plt.xlim(-1.5, 1.5)\n",
    "    plt.ylim(-1.5, 1.5)\n",
    "\n",
    "    plt.scatter(pca_synth.iloc[:, 0], pca_synth.iloc[:, 1],\n",
    "                c='red', alpha=0.5, label='Synthetic', s=100)\n",
    "    plt.xlim(-1.5, 1.5)\n",
    "    plt.ylim(-1.5, 1.5)\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "    ax2 = fig.add_subplot(spec[0,1])\n",
    "    ax2.set_title('TSNE results',\n",
    "                  fontsize=20,\n",
    "                  color='red',\n",
    "                  pad=10)\n",
    "\n",
    "    # t-SNE 散点图\n",
    "    plt.scatter(tsne_results.iloc[:shape, 0].values, tsne_results.iloc[:shape, 1].values,\n",
    "                c='black', alpha=0.5, label='Original')\n",
    "    plt.scatter(tsne_results.iloc[shape:, 0], tsne_results.iloc[shape:, 1],\n",
    "                c='red', alpha=0.5, label='Synthetic')\n",
    "\n",
    "    ax2.legend()\n",
    "\n",
    "    fig.suptitle('Validating synthetic vs real data diversity and distributions',\n",
    "                 fontsize=16,\n",
    "                 color='grey')\n",
    "    \n",
    "    save_path = os.path.join(output_folder, 'img')\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    png_filename = f\"PCA_{file_name}.png\"\n",
    "    save_path = os.path.join(save_path, png_filename)\n",
    "    plt.savefig(save_path, dpi=200)\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95752243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成数据的种类\n",
    "id = 'lu'\n",
    "\n",
    "# 输入文件夹路径\n",
    "input_folder = os.path.join('synth_data', id)\n",
    "\n",
    "# 输出文件夹路径\n",
    "output_folder = os.path.join('process_data', id)\n",
    "\n",
    "# 循环读取文件并处理\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".csv\"):  # 确保只读取CSV文件\n",
    "        filepath = os.path.join(input_folder, filename)\n",
    "        df = load_data(filepath, encode='utf-8')\n",
    "        # 获取原始文件的文件名（不包含扩展名）\n",
    "        file_name = os.path.splitext(filename)[0]\n",
    "        filtered_df = remove_multi(df)\n",
    "        filtered_df = add_col(filtered_df)\n",
    "        filtered_df = add_weather(filtered_df)\n",
    "        filtered_df = transform_time(filtered_df)\n",
    "        filtered_df = add_course(filtered_df, id)\n",
    "        filtered_df = transform_merchant(filtered_df)\n",
    "        filtered_df = draw_plot(filtered_df, id, output_folder, file_name)\n",
    "        PCA_tSNE(filtered_df, id, output_folder, file_name)\n",
    "        save_path = os.path.join(output_folder, filename)\n",
    "        filtered_df.to_csv(save_path , sep= ',', encoding='utf-8',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
